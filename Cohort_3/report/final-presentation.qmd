---
title: "The Opportunity Project for Cities"
subtitle: "Cohort 3 Impact Evaluation Report 2023"
format:
  revealjs:
    slide-number: true
    show-slide-number: all
    chalkboard: 
      buttons: false
    preview-links: auto
    logo: images/topc-logo.png
    footer: Published on January 2, 2024
editor: visual
---

## Acknowledgement

We would like to thank the Knight Foundation, the Beeck Center for Social Impact and Innovation, Center for Public Impact, Google.org, and all participating city governments and their community partners.

Your active engagement, insights, and commitment were pivotal in bringing this report to fruition.

![](images/topc-partners.png)

::: footer
Learn more: [TOPC Partners](https://quarto.org/docs/output-formats/html-code.html#highlighting)
:::

## Executive Summary

## **Overview**

This report provides insights and recommendations into the learning outcomes of Cohort 3 participants, city governments, in The Opportunity Project for Cities (TOPC) in 2023. It evaluates the program's effectiveness in improving participants':

-   Awareness and knowledge.

-   Competencies and confidence (self-efficacy).

-   Recognition and application of methods.

-   Intention to action and overall satisfaction.

## **Program Goals**

The primary objectives of the TOPC program are to: 

-   Understand and strengthen human-centered design, open data, and design sprint principles and methods across city governments.

-   Ensure grounding in human-centered design concepts through peer-to-peer connections and community partnerships.

-   Share practical strategies and approaches for solving a local challenge and implementing data-driven solution with the community.

## **Success Outcomes**

This report aims to evaluates the first outcome from these outcome measures:

1.  Increase in public servants' capacity for community-centered digital innovation.

2.  More effective government services and increased data use.

3.  Improved government legitimacy.

::: footer
Learn more: [Program Outcomes](https://quarto.org/docs/output-formats/html-code.html#highlighting)
:::

## Evaluation Goals

The evaluation focuses on comprehensively assessing the TOPC design sprint's effectiveness across all participant cohorts. The key goals include:

-   **Goal 1:** Increase participants' **awareness and knowledge** of human-centered design principles regarding addressing local community civic challenges.

-   **Goal 2:** Increase participant **competencies and confidence** in the intersections of human-centered design methods, community research, product stewardship, and partnerships for addressing local community challenges.

------------------------------------------------------------------------

-   **Goal 3**: Increase participants' **recognition and application** of human-centered design and design sprint techniques in their work in collaboration with their department teams, community partners, and residents.

-   **Goal 4:** Measure **participant satisfaction** with program outcomes and **intention toward action** after the design sprint.

## Evaluation Questions

The evaluation questions that drive the analysis, insights, and recommendations are:

-   **Goal 1:** To what extent has the program improved participants' awareness and knowledge of human-centered design in a local challenge in their communities?

-   **Goal 2:** How have participants' competencies and confidence in understanding and applying the concepts of community research, product stewardship, and partnership improved to addressing local community challenges?

------------------------------------------------------------------------

-   **Goal 3**: How has the program increased participants' ability to recognize and apply methods and concepts in human-centered design and design sprints?

-   **Goal 4:** How satisfied are participants with the outcomes of the program? What are the participants' intentions regarding applying the knowledge and skills gained in the program, particularly after the end of the program?

## Logic Model

TBD

## Methodology

There is a total of **40 city governments employees** who participated in Cohort 3 of the TOPC program in 2023. **9-15 participants** (22.5-37.5%) engaged in evaluation activities.

To measure baseline knowledge, inform course corrections, change in participant learning, and change in department's work, the data collection methods are:

-   **Structured evaluations** through surveys and interviews at specific points in the program.

-   **Ongoing feedback** with an Insights and testimonials form throughout the design sprint.

------------------------------------------------------------------------

| Survey            | Sample Size | Timing                               |
|-------------------|-------------|--------------------------------------|
| Pre-test          | 15          | Before program                       |
| Midpoint check-in | 9           | After 2-3 phases                     |
| Post-test         | 13          | At the end of program                |
| 6-month follow-up | N/A         | 6-months after program               |
| Learning session  | 4 - 18      | At the end of every learning session |

Show composition of pre vs post surveys based on city government, role, gender, seniority, etc. Show total sample size + weighted sample

Show composition of interview participants based on city government, role, gender, seniority, etc.

| Interview | Sample Size | Timing                |
|-----------|-------------|-----------------------|
| Post-test | 13          | At the end of program |

## Insights

#### 1. Participant awareness and knowledge

#### 2. Participant competencies and confidence (self-efficacy)

#### 3. Participant recognition and application of methods

#### 4. Participant satisfaction and Intention to Action

------------------------------------------------------------------------

::: panel-tabset
## Summary

The survey results suggest a positive shift in participants' self-reported familiarity with community research skills after the program, especially in Macon where all participants agreed unanimously on this skill. The results indicate the curriculum's effectiveness in equipping individuals with practical community research skills, regardless of their initial familiarity.

The interviews TBD.

## Before

![](images/q8_pre_cross_tab_city_plot.png){fig-align="center" width="90%"}

## After

![](images/q4_post_cross_tab_city_plot-02.png){fig-align="center" width="80%"}

## Quotes
:::

::: notes
The program has potential for replication and adaptation in diverse settings, emphasizes the importance of customizing educational approaches to cater to varying levels of baseline knowledge, to benefit broader community research initiatives through skilled practitioners.

While these responses indicate perceived effectiveness of the program, without inferential statistical analysis, we cannot definitively conclude the program's impact. The data highlights the value of such training initiatives and supports the need for further analysis to understand the true extent of learning outcomes across different starting skill levels.
:::

::: footer
KLA: Community research
:::

------------------------------------------------------------------------

::: panel-tabset
## Summary

Survey data indicates participants from all cities perceived an improvement in their ability to apply user testing feedback in developing solutions after the program, especially in Miami where the belief in skill enhancement was strongest. This perceived improvement, particularly where initial self-assessed expertise was low, suggests the program was effective in bridging knowledge gaps. It underscores the importance of practical, hands-on training in user testing methods, which can empower participants to more effectively iterate on community-based projects, enhancing the relevance and impact of their work.

## Before

![](images/q9_pre_cross_tab_city_plot.png){fig-align="center" width="80%"}

## After

![](images/q6_post_cross_tab_city_plot-01.png){fig-align="center" width="80%"}

## Data

Weighted summary statistics. Median (standard deviation).

::: callout-caution
The weighted pre- and post-test summary statistics are not comparable because their questions and Likert scales are different. But, they provide general trends and patterns that are useful.
:::

| City    | Pre | Post         |
|---------|-----|--------------|
| Akron			   | 2   | 1.380131119  |
| Detroit | 3   | NA           |
| Macon   | 3   | 0.5773502692 |
| Miami   | 4   | 0.5773502692 |

## Quotes
:::

::: notes
:::

::: footer
KLA: User Testing
:::

## Recommendations {data-link="Recommendations"}

## Limitations

::: callout-warning
## Key Takeaway

The analysis, while supported by survey sampling weights, comes with inherent limitations. While these weights and other statistical techniques help make the sample more reflective of the broader population, it's important to note that they cannot entirely compensate for all the limitations inherent in the data.

To interpret the findings within the context of these methodological constraints, qualitative data from interviews provide the nuances of uncovering participant experiences.

We strive for transparency and rigor, ensuring stakeholders are fully informed about the strengths and constraints of the findings.
:::

------------------------------------------------------------------------

-   **Survey design data collection challenges:** The pre- and post-test surveys had question design errors which contributed to bias. Thus, the evaluation relied on descriptive data analysis and interviews for identifying trends and themes. Qualitative analysis of open-ended responses provided insights into changes in participants' experiences and perceptions.

-   **Survey distribution and sample representation:** Incomplete or varying methods of survey distribution may have impacted the representativeness of results. Application of survey design weights attempted to correct for potential biases in sample size and results.

------------------------------------------------------------------------

-   **Challenges in data analysis:** Presence of a single primary sampling unit (PSU) in post-test data limited variance calculation. Alternatives like bootstrapping or jackknife methods were not used for causal inference due to it not being able to correct for existing bias in data collection.

-   **Comparison of pre-test and post-test data:** Pre-test and post-test surveys are conceptually aligned but differed in both questions and scales: a 4-point Likert scale for pre-test (focusing on agreement) and a 5-point scale for post-test (assessing expertise levels). Shifts in response distributions were analyzed using non-parametric methods, focusing on overall trends with descriptive statistics to compare before and after without inferring statistical significance. They are not directly comparable, which means means a direct comparison or change analysis like a t-test is not straightforward.

------------------------------------------------------------------------

-   **Nonresponse and uneven survey uptake across cities:** Response rates varied notably, with about 15 responses for the pre-test and 13 for the post-test. Uneven distribution of responses across cities, such as only one pre-test response from a specific city, complicated the analysis.

-   **Participant drop offs and inconsistent engagement:** Role or organizational changes among participants led to data gaps, as those no longer in the program couldn't provide follow-up information.

------------------------------------------------------------------------

-   In our analysis of the survey data, we have applied survey sampling weights to adjust for disparities in response rates and representation across different city government groups. While these weights help in making our sample more reflective of the broader population, it's important to note that they cannot entirely compensate for all the limitations inherent in the data. Specifically, the non-random nature of our sampling method and the presence of biases in sample collection present challenges that weights alone may not fully address. As a result, while the weighted analysis offers valuable insights, the findings should be interpreted with an understanding of these constraints. We remain committed to transparency and rigor in our research and want to ensure that all stakeholders are aware of the nuances and limitations of the methods employed.

## FAQ

## Appendix

-   Github repository

-   Survey responses

-   Interview recordings and analysis

## References

## Contact Us

If you have questions or comments about this report, email Elham Ali, Researcher at the Beeck Center for Social Impact and Innovation, at [[elham.ali\@georgetown.edu]{.underline}](http://elham.ali@georgetown.edu/). 
